## Machine Learning Foundations

### When Can Machines Learn?

Lecture   |Topic   |Content   
:---|:---|:---
Week 1 |The Learning Problem|1. What is Machine Learning<br>2. Applications of Machine Learning<br>3. Components of Machine Learning<br>4. Machine Learning and Other Fields
Week 2 |Learning to Answer Yes/No|1. Perceptron Hypothesis Set<br>2. Perceptron Learning Algorithm (PLA)<br>3. Guarantee of PLA<br>4. Non-Separable Data
Week 3 |Types of Learning|1. Learning with Different Output Space<br>2. Learning with Different Data Label<br>3. Learning with Different Protocol<br>4. Learning with Different Input Space
Week 4 |Feasibility of Learning|1. Learning is Impossible?<br>2. Probability to the Rescue<br>3. Connection to Learning<br>4. Connection to Real Learning

### Why Can Machines Learn?

Lecture   |Topic   |Content   
:---|:---|:---
Week 5 |Training versus Testing|1. Recap and Preview<br>2. Effective Number of Lines<br>3. Effective Number of Hypotheses<br>4. Break Point
Week 6 |Theory of Generalization|1. Restriction of Break Point<br>2. Bounding Function: Basic Cases<br>3. Bounding Function: Inductive Cases<br>4. A Pictorial Proof
Week 7 |The VC Dimension|1. Definition of VC Dimension<br>2. VC Dimension of Perceptrons<br>3. Physical Intuition of VC Dimension<br>4. Interpreting VC Dimension
Week 8 |Noise and Error|1. Noise and Probabilistic Target<br>2. Error Measure<br>3. Algorithmic Error Measure<br>4. Weighted Classification

### How Can Machines Learn?

Lecture   |Topic   |Content   
:---|:---|:---
Week 9 |Linear Regression|1. Linear Regression Problem<br>2. Linear Regression Algorithm<br>3. Generalization Issue<br>4. Linear Regression for Binary Classification
Week 10 |Logistic Regression|1. Logistic Regression Problem<br>2. Logistic Regression Error<br>3. Gradient of Logistic Regression Error<br>4. Gradient Descent
Week 11 |Linear Models for Classification|1. Linear Models for Binary Classification<br>2. Stochastic Gradient Descent<br>3. Multiclass via Logistic Regression<br>4. Multiclass via Binary Classification
Week 12 |Nonlinear Transformation|1. Quadratic Hypotheses<br>2. Nonlinear Transform<br>3. Price of Nonlinear Transform<br>4. Structured Hypothesis Sets

### How Can Machines Learn Better?

Lecture   |Topic   |Content   
:---|:---|:---
Week 13 |Hazard of Overfitting|1. What is Overfitting?<br>2. The Role of Noise and Data Size<br>3. Deterministic Noise<br>4. Dealing with Overfitting
Week 14 |Regularization|1. Regularized Hypothesis Set<br>2. Weight Decay Regularization<br>3. Regularization and VC Theory<br>4. General Regularizers
Week 15 |Validation|1. Model Selection Problem<br>2. Validation<br>3. Leave-One-Out Cross Validation<br>4. V-Fold Cross Validation
Week 16 |Three Learning Principles|1. Occam's Razor<br>2. Sampling Bias<br>3. Data Snooping<br>4. Power of Three

## Machine Learning Techniques

### Embedding Numerous Features

Lecture   |Topic   |Content   
:---|:---|:---
Week 1 |Linear Support Vector Machine|1. Large-Margin Separating Hyperplane<br>2. Standard Large-Margin Problem<br>3. Support Vector Machine<br>4. Reasons behind Large-Margin Hyperplane
Week 2 |Dual Support Vector Machine|1. Motivation of Dual SVM<br>2. Lagrange Dual SVM<br>3. Solving Dual SVM<br>4. Messages behind Dual SVM
Week 3 |Kernel Support Vector Machine|1. Kernel Trick<br>2. Polynomial Kernel<br>3. Gaussian Kernel<br>4. Comparison of Kernels
Week 4 |Soft-margin Support Vector Machine|1. Motivation and Primal Problem<br>2. Dual Problem<br>3. Messages behind Soft-Margin SVM<br>4. Model Selection
Week 5 |Kernel Logistic Regression|1. Soft-Margin SVM as Regularized Model<br>2. SVM versus Logistic Regression<br>3. SVM for Soft Binary Classification<br>4. Kernel Logistic Regression
Week 6 |Support Vector Regression|1. Kernel Ridge Regression<br>2. Support Vector Regression Primal<br>3. Support Vector Regression Dual<br>4. Summary of Kernel Models

### Combining Predictive Features

Lecture   |Topic   |Content   
:---|:---|:---
Week 7 |Blending and Bagging|1. Motivation of Aggregation<br>2. Uniform Blending<br>3. Linear and Any Blending<br>4. Bagging (Bootstrap Aggregation)
Week 8 |Adaptive Boosting|1. Motivation of Boosting<br>2. Diversity by Re-weighting<br>3. Adaptive Boosting Algorithm<br>4. Adaptive Boosting in Action
Week 9 |Decision Tree|1. Decision Tree Hypothesis<br>2. Decision Tree Algorithm<br>3. Decision Tree Heuristics in C&RT<br>4. Decision Tree in Action
Week 10 |Random Forest|1. Random Forest Algorithm<br>2. Out-Of-Bag Estimate<br>3. Feature Selection<br>4. Random Forest in Action
Week 11 |Gradient Boosted Decision Tree|1. Adaptive Boosted Decision Tree<br>2. Optimization View of AdaBoost<br>3. Gradient Boosting<br>4. Summary of Aggregation Models

### Distilling Hidden Features

Lecture   |Topic   |Content   
:---|:---|:---
Week 12 |Neural Network|1. Motivation<br>2. Neural Network Hypothesis<br>3. Neural Network Learning<br>4. Optimization and Regularization
Week 13 |Deep Learning|1. Deep Neural Network<br>2. Autoencoder<br>3. Denoising Autoencoder<br>4. Principal Component Analysis
Week 14 |Radial Basis Function Network|1. RBF Network Hypothesis<br>2. RBF Network Learning<br>3. k-Means Algorithm<br>4. k-Means and RBF Network in Action
Week 15 |Matrix Factorization|1. Linear Network Hypothesis<br>2. Basic Matrix Factorization<br>3. Stochastic Gradient Descent<br>4. Summary of Extraction Models
Week 16 |Final|1. Feature Exploitation Techniques<br>2. Error Optimization Techniques<br>3. Overfitting Elimination Techniques<br>4. Machine Learning in Practice

